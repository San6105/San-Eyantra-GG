# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GAbaRuyURF-Q0k_y6YpSUWo-YeXndhht
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
from sklearn.model_selection import GridSearchCV

# Define the preprocessing function
data_frame='/content/task_1a_dataset1.csv'
def preprocessing(data_frame):
    ds = pd.read_csv(data_frame)
    cat_features = ds.select_dtypes(include=["object"]).columns
    encoder = ds.drop_duplicates()
    ds.fillna(ds.mean(), inplace=True)  # Impute missing values in numerical columns with mean
    ds.fillna(ds.mode().iloc[0], inplace=True)  # Impute missing values in categorical columns with mode

    le = LabelEncoder()
    for column in cat_features:
        encoder[column] = le.fit_transform(encoder[column])
    return encoder

# Define the feature selection function
def feature_selection(dataframe):
    target = dataframe['LeaveOrNot']
    features = dataframe.drop(columns=['LeaveOrNot'])
    return features, target

# Define the function to load data as tensors
def load_as_tensors(features_and_targets, batch_size=64, validation_split=0.2, shuffle=True):
    features, targets = features_and_targets

    X = torch.tensor(features.values, dtype=torch.float32)
    y = torch.tensor(targets.values, dtype=torch.int64)  # Use int64 for labels

    num_validation = int(validation_split * len(features))

    X_train_tensor = X[:-num_validation]
    y_train_tensor = y[:-num_validation]
    X_val_tensor = X[-num_validation:]
    y_val_tensor = y[-num_validation:]

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    tensors_and_iterable_training_data = [X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, train_loader, val_loader]
    return tensors_and_iterable_training_data

# Define your neural network model with regularization
class LeaveOrNot(nn.Module):
    def __init__(self):
        super(LeaveOrNot, self).__init__()
        self.fc1 = nn.Linear(8, 64)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(64, 8)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(8, 64)
        self.relu3 = nn.ReLU()
        self.fc4 = nn.Linear(64, 2)


    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        x = self.relu3(x)
        x = self.fc4(x)

        return x

# Define the loss function
def model_loss_function(outputs, labels):
    loss_function = nn.CrossEntropyLoss()
    return loss_function(outputs, labels)

# Define the optimizer with L2 regularization
def model_optimizer(model, learning_rate=0.05, weight_decay=0.001):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    return optimizer

# Define the number of epochs
def model_number_of_epochs():
    number_of_epochs =5
    return number_of_epochs

# Training loop with learning rate scheduler
def train_model(model, train_loader, optimizer, loss_fn, num_epochs):
    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Learning rate scheduler
    for epoch in range(num_epochs):
        scheduler.step()
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, labels.long())
            loss.backward()
            optimizer.step()

        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

# Define a validation function
def validation_function(trained_model, val_loader):
    trained_model.eval()
    total_accuracy = 0.0
    total_samples = 0

    for inputs, labels in val_loader:
        outputs = trained_model(inputs)
        _, predicted = torch.max(outputs, 1)
        correct = (predicted == labels).sum().item()
        total_samples += labels.size(0)
        total_accuracy += correct

    model_accuracy = total_accuracy / total_samples
    print(y_val_tensor.shape)
    print('predicted',predicted.shape)
    print('total_accuracy',total_accuracy)
    print( 'total_samples', total_samples)
    print(f'Validation Accuracy: {model_accuracy:.4f}')
    return model_accuracy

# Main code
if __name__ == "__main__":
    # Preprocess the data
    s = preprocessing("/content/task_1a_dataset1.csv")
    features_and_targets = feature_selection(s)

    # Load data as tensors
    tensors_and_iterable_training_data = load_as_tensors(features_and_targets)
    X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, train_loader, val_loader = tensors_and_iterable_training_data

    # Create and train the model with regularization
    model = LeaveOrNot()
    loss_function = model_loss_function  # Use the loss function directly
    optimizer = model_optimizer(model)
    num_epochs = model_number_of_epochs()
    train_model(model, train_loader, optimizer, loss_function, num_epochs)

    # Save the trained model
    torch.save(model.state_dict(), 'trained_model.pth')

    # Validate the trained model
    trained_model = LeaveOrNot()
    trained_model.load_state_dict(torch.load('trained_model.pth'))
    validation_function(trained_model, val_loader)

    # Add callback functions and pruning as needed
    # You can extend the code here for callbacks and pruning based on your requirements